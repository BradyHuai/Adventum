{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BradyHuai/Adventum/blob/master/5-conv2d_dw_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eGZIh3o1xch"
      },
      "source": [
        "# Depthwise-seperable 2D Convolution on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD2GHA5WVSow"
      },
      "source": [
        "## 1. Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HnmfRsiBVSow",
        "outputId": "494a1fec-270f-4008-b655-fdfa44b3aa57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rR5CmYuCVSox"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username\n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AZHBLuQfVSox",
        "outputId": "14386851-f3ed-472a-b630-80107b5f7fa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/ece5545’: File exists\n",
            "/content/gdrive/MyDrive/ece5545\n",
            "fatal: destination path 'a3-BradyHuai' already exists and is not an empty directory.\n",
            "/content/gdrive/MyDrive/ece5545/a3-BradyHuai\n",
            "M\tsrc/ops.py\n",
            "Already on 'main'\n",
            "Your branch and 'origin/main' have diverged,\n",
            "and have 1 and 1 different commits each, respectively.\n",
            "  (use \"git pull\" to merge the remote branch into yours)\n",
            "\u001b[33mhint: You have divergent branches and need to specify how to reconcile them.\u001b[m\n",
            "\u001b[33mhint: You can do so by running one of the following commands sometime before\u001b[m\n",
            "\u001b[33mhint: your next pull:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
            "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
            "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
            "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
            "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
            "\u001b[33mhint: invocation.\u001b[m\n",
            "fatal: Need to specify how to reconcile divergent branches.\n",
            "/content/gdrive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a3-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a3-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a3-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfJNPVatVSox",
        "outputId": "f047a589-ab20-42d0-b768-d565044841d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7Bztq-VVSoy",
        "outputId": "1266e8e7-a4f1-460a-90f9-243887746e5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-conv1d_cpu.ipynb  3-conv1d_fpga.ipynb  5-conv2d_dw_gpu.ipynb\tREADME.md  tests\n",
            "2-conv1d_gpu.ipynb  4-gemm_gpu.ipynb\t leaderboard_id.txt\tsrc\n"
          ]
        }
      ],
      "source": [
        "!ls {PROJECT_ROOT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EghxXWHSVyZR"
      },
      "source": [
        "## 2 Install TVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phf10Wb1VxoB",
        "outputId": "7d25bcb4-97a4-4bd5-d95b-1f1688ad972f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://tlcpack.ai/wheels\n",
            "Collecting tlcpack-nightly-cu102\n",
            "  Downloading https://github.com/tlc-pack/tlcpack/releases/download/v0.12.dev/tlcpack_nightly_cu102-0.15.dev118%2Bg51bdaec6e-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.5/428.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (25.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (3.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (2.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (1.14.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (6.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from tlcpack-nightly-cu102) (4.13.1)\n",
            "Installing collected packages: tlcpack-nightly-cu102\n",
            "Successfully installed tlcpack-nightly-cu102-0.15.dev118+g51bdaec6e\n"
          ]
        }
      ],
      "source": [
        "!pip install tlcpack-nightly-cu102 -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "-CPQDzpNhaA2",
        "outputId": "38f786d0-d0bf-4855-a9cb-c8348d77d1b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "Successfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.setrecursionlimit(10000000)"
      ],
      "metadata": {
        "id": "ba-6B215hbck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1dTOTtqVSoy"
      },
      "source": [
        "## 3. Implement `make_dwsp_conv2d_gpu_scheduler` function in `src.ops`\n",
        "\n",
        "In that function, you are required to implemented 2D convolution and use TVM to optimize it.\n",
        "Please use zero padding and unit stride.\n",
        "You can assume kernel size to be an odd number.\n",
        "The padding will equals to kernel size minus ones.\n",
        "In this case, the output image will preserve the input image dimension.\n",
        "\n",
        "The `make_dwsp_conv2d_gpu_scheduler` takes following arguments:\n",
        "1. Batch size $B$;\n",
        "2. Input channel size $C$;\n",
        "3. Input image height $H$;\n",
        "4. Input image width $W$;\n",
        "5. Output number of channels $O$;\n",
        "6. Kernel size $K$\n",
        "\n",
        "You should return both the TVM scheduler and the TVM opterator for\n",
        "1. Input tensor $x$ with size (B, C, H, W)\n",
        "2. Input kernel weight $y$ with size (O, 1, K, K)\n",
        "3. Output $out$ with size (B, O, H, W)\n",
        "\n",
        "The scheduler should be able to used to build a function with signature $func(x, y, out)$.\n",
        "Please see the following cells the usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rbp2mmTVSoz",
        "outputId": "c1ee6678-c240-4100-8dd3-556edf7fcc97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: [[[[ 3.3276675  4.8995895  6.215834  ...  4.4062138  4.0085883\n",
            "     3.4619527]\n",
            "   [ 4.218955   6.305473   7.5448966 ...  6.680538   5.1722975\n",
            "     4.1840568]\n",
            "   [ 5.9705114  7.1710777  9.073423  ...  8.2471485  5.9428086\n",
            "     5.8443937]\n",
            "   ...\n",
            "   [ 6.224985   7.59454    8.99978   ... 10.204355   8.168089\n",
            "     7.530574 ]\n",
            "   [ 5.340989   5.7137794  7.9899516 ...  9.435118   7.291466\n",
            "     6.326056 ]\n",
            "   [ 5.5450606  4.8329124  6.316245  ...  8.300325   5.673406\n",
            "     4.4685507]]\n",
            "\n",
            "  [[ 3.4801044  4.617272   5.5128174 ...  3.9814734  3.3263366\n",
            "     2.3233905]\n",
            "   [ 4.7848625  6.169961   8.493502  ...  5.746636   5.2827597\n",
            "     3.3354123]\n",
            "   [ 5.810236   7.881957  10.092031  ...  6.555023   5.776548\n",
            "     4.225312 ]\n",
            "   ...\n",
            "   [ 5.153788   8.277957   8.574255  ...  9.321503   8.891026\n",
            "     6.355317 ]\n",
            "   [ 4.8645654  7.141524   8.083791  ...  8.101811   7.695097\n",
            "     5.866612 ]\n",
            "   [ 4.3487816  5.14711    7.46345   ...  6.7300224  6.4446535\n",
            "     3.9061525]]\n",
            "\n",
            "  [[ 4.7784367  6.5943384  7.5576344 ...  6.2125688  5.7941084\n",
            "     5.1093326]\n",
            "   [ 5.4014974  6.7814283  8.409281  ...  7.4084783  7.109019\n",
            "     6.4732237]\n",
            "   [ 7.0225782  8.511673  11.684382  ...  9.355738   8.52458\n",
            "     8.073342 ]\n",
            "   ...\n",
            "   [ 6.7970347  8.116599  10.5083685 ...  9.044154   7.665516\n",
            "     5.955664 ]\n",
            "   [ 5.9124904  6.3656387  8.352624  ...  8.379883   6.218607\n",
            "     4.7096815]\n",
            "   [ 4.4067426  4.6123033  6.456919  ...  6.7782507  5.793043\n",
            "     4.041332 ]]\n",
            "\n",
            "  [[ 4.5061107  5.7766824  7.320362  ...  7.258332   6.1402407\n",
            "     4.5184236]\n",
            "   [ 6.816869   8.302731   9.046025  ...  8.660421   7.64192\n",
            "     5.375962 ]\n",
            "   [ 7.602437   9.147937   9.833019  ... 10.8954735  8.459318\n",
            "     7.2460628]\n",
            "   ...\n",
            "   [ 6.3878875  8.661015   9.080041  ... 10.972168   7.426839\n",
            "     6.8973355]\n",
            "   [ 4.790215   5.5235066  8.01911   ...  8.871018   7.4119744\n",
            "     5.185291 ]\n",
            "   [ 3.54469    4.4801054  5.353401  ...  7.260677   5.431677\n",
            "     4.5414867]]]\n",
            "\n",
            "\n",
            " [[[ 2.9436867  4.183313   5.535042  ...  6.8316455  6.0534387\n",
            "     4.360797 ]\n",
            "   [ 2.8424592  5.1756854  6.3039937 ... 10.136975   7.771319\n",
            "     4.97582  ]\n",
            "   [ 5.1213775  6.0011253  7.9431896 ... 11.315271   9.382066\n",
            "     6.7982388]\n",
            "   ...\n",
            "   [ 5.8627486  6.8339453 10.105399  ... 11.125214   8.573317\n",
            "     7.2151513]\n",
            "   [ 4.9323926  6.80953    8.304703  ...  9.257949   6.42372\n",
            "     6.294381 ]\n",
            "   [ 4.187996   5.14197    6.6152124 ...  8.3691     6.2303777\n",
            "     4.5076222]]\n",
            "\n",
            "  [[ 3.320269   4.823156   5.619832  ...  6.3851256  4.8048377\n",
            "     3.689139 ]\n",
            "   [ 4.9040227  6.6977606  7.873691  ...  8.28133    7.7021346\n",
            "     5.3660326]\n",
            "   [ 5.0218053  8.620149   8.237691  ... 10.142753   8.553048\n",
            "     6.5375504]\n",
            "   ...\n",
            "   [ 6.7458262  9.178233   9.36494   ...  8.833257   8.028845\n",
            "     6.022053 ]\n",
            "   [ 4.69817    7.7331038  7.6046796 ...  8.576668   6.5270824\n",
            "     5.6788955]\n",
            "   [ 4.094483   4.1511397  5.381549  ...  7.4288344  5.944191\n",
            "     3.8939443]]\n",
            "\n",
            "  [[ 5.0658307  5.632349   6.997943  ...  6.5592966  7.655088\n",
            "     5.3452096]\n",
            "   [ 4.875606   6.179392   8.135044  ...  8.930559   8.500064\n",
            "     6.818972 ]\n",
            "   [ 6.5941005  7.488073   9.972958  ... 11.913118  11.233344\n",
            "     8.794994 ]\n",
            "   ...\n",
            "   [ 6.8612394  8.966171   9.284542  ...  8.972813   8.056106\n",
            "     6.246389 ]\n",
            "   [ 5.257062   7.126366   7.513833  ...  7.5627418  7.0701494\n",
            "     5.610976 ]\n",
            "   [ 4.467593   5.305904   5.502425  ...  5.7554727  4.5707846\n",
            "     3.7413795]]\n",
            "\n",
            "  [[ 5.2422423  6.464194   7.415547  ...  7.9280167  6.4814196\n",
            "     5.3490686]\n",
            "   [ 6.728529   9.323437  10.929029  ... 11.008582   8.757357\n",
            "     6.5275826]\n",
            "   [ 7.806283   8.971714  10.2767315 ... 11.981644  10.119192\n",
            "     7.644429 ]\n",
            "   ...\n",
            "   [ 8.462947   9.553811  12.008583  ... 11.565642   9.209108\n",
            "     7.476376 ]\n",
            "   [ 6.374058   7.4657598 10.833393  ... 10.355004   9.943098\n",
            "     6.481892 ]\n",
            "   [ 5.2897563  7.030411   8.559684  ...  7.6460986  6.3579836\n",
            "     4.709479 ]]]\n",
            "\n",
            "\n",
            " [[[ 4.808472   6.191932   7.375732  ...  5.342011   5.7754498\n",
            "     4.121672 ]\n",
            "   [ 6.0281115  6.015726   9.269289  ...  6.974413   6.0815005\n",
            "     5.2838306]\n",
            "   [ 6.469217   7.633305   9.872742  ...  9.779166   8.676399\n",
            "     5.6010923]\n",
            "   ...\n",
            "   [ 5.4992647  6.85733    8.823612  ... 10.31822    8.63488\n",
            "     7.0819597]\n",
            "   [ 5.4913764  6.857488   8.104744  ...  8.075174   7.2666874\n",
            "     5.1145716]\n",
            "   [ 4.924723   6.0976357  6.4517794 ...  7.059266   5.4228325\n",
            "     4.687116 ]]\n",
            "\n",
            "  [[ 2.346499   3.885155   4.647148  ...  4.72985    5.4325795\n",
            "     3.5926766]\n",
            "   [ 4.0160594  4.8178453  6.7810426 ...  7.687932   7.389682\n",
            "     5.479627 ]\n",
            "   [ 4.158042   6.065784   8.494688  ...  9.084799   9.358534\n",
            "     5.3618655]\n",
            "   ...\n",
            "   [ 7.293386   8.796123  10.71113   ...  8.194755   7.54126\n",
            "     6.346324 ]\n",
            "   [ 6.096318   8.263836   8.903085  ...  6.281964   6.2612042\n",
            "     4.2692757]\n",
            "   [ 4.9588356  6.1769605  6.5146585 ...  5.9041095  4.9993353\n",
            "     4.0313497]]\n",
            "\n",
            "  [[ 3.9175932  5.6647654  6.3950872 ...  6.5416064  5.770948\n",
            "     4.182428 ]\n",
            "   [ 4.930884   6.9864025  8.472519  ...  6.5177255  6.9585037\n",
            "     4.5863123]\n",
            "   [ 7.321303   7.985826  11.724563  ...  9.928083   9.645745\n",
            "     7.258204 ]\n",
            "   ...\n",
            "   [ 8.169255   8.400118  10.4956    ...  8.718231   7.7030973\n",
            "     5.9511685]\n",
            "   [ 5.710099   6.500845   8.520419  ...  7.6950903  6.880166\n",
            "     5.522216 ]\n",
            "   [ 4.5985694  5.6359215  7.4890475 ...  5.5995994  5.0728245\n",
            "     3.7827318]]\n",
            "\n",
            "  [[ 3.6902306  6.268184   6.957672  ...  8.126771   5.5090637\n",
            "     4.7106915]\n",
            "   [ 5.8518877  7.4653006  8.741173  ... 10.259149   8.169714\n",
            "     4.8047853]\n",
            "   [ 5.4116316  7.237217  10.273778  ... 10.244263   9.527752\n",
            "     6.2810383]\n",
            "   ...\n",
            "   [ 6.7649508  8.855792  10.20539   ...  9.64896    8.6839\n",
            "     6.5418077]\n",
            "   [ 6.257114   7.435243   9.902451  ...  9.163046   6.969471\n",
            "     6.328724 ]\n",
            "   [ 4.588844   5.887635   7.2878585 ...  6.6763124  5.128954\n",
            "     3.9499285]]]]\n",
            "2DConv TVM: 0.118239 ms\n"
          ]
        }
      ],
      "source": [
        "import tvm\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import sys\n",
        "# Adding assignment 3 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "from src.ops import make_dwsp_conv2d_gpu_scheduler\n",
        "\n",
        "B = 3\n",
        "C = 4\n",
        "H = 16\n",
        "W = 32\n",
        "K = 7\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(B, C, H, W).astype(dtype)\n",
        "w_np = np.random.rand(C, 1, K, K).astype(dtype)\n",
        "\n",
        "s, inp, ker, out = make_dwsp_conv2d_gpu_scheduler(B, C, H, W, K)\n",
        "func = tvm.build(s, [inp, ker, out], \"cuda\")\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a = tvm.nd.array(a_np, dev)\n",
        "w = tvm.nd.array(w_np, dev)\n",
        "b = tvm.nd.array(np.zeros((B, C, H, W), dtype), dev)\n",
        "func(a, w, b)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1, repeat =1)\n",
        "\n",
        "print(\"Output:\", b)\n",
        "print(f\"2DConv TVM: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import timeit\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def make_func(*args):\n",
        "    s, A, W, O = make_dwsp_conv2d_gpu_scheduler(*args)\n",
        "    func = tvm.build(s, [A, W, O], \"cuda\")\n",
        "    return func\n",
        "\n",
        "def ans_torch(a_torch, w_torch):\n",
        "    B, C, H, W = a_torch.size()\n",
        "    O, D, K1, K2 = w_torch.size()\n",
        "    assert K1 == K2\n",
        "    assert D == 1\n",
        "    K = K1\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    b_torch = F.conv2d(\n",
        "        a_torch, w_torch, bias=None, stride=1,\n",
        "        padding=((K - 1)//2), dilation=1, groups=C)\n",
        "    torch.cuda.synchronize()\n",
        "    return b_torch\n",
        "\n",
        "# Define dimension\n",
        "B = 3\n",
        "C = 4\n",
        "H = 16\n",
        "W = 32\n",
        "K = 7\n",
        "dtype = 'float32'\n",
        "np.random.seed(seed=1024)\n",
        "a_np = np.random.rand(B, C, H, W).astype(dtype)\n",
        "w_np = np.random.rand(C, 1, K, K).astype(dtype)\n",
        "\n",
        "n_repeat = 100\n",
        "\n",
        "# Torch input\n",
        "a_torch = torch.tensor(a_np).float()\n",
        "w_torch = torch.tensor(w_np).float()\n",
        "\n",
        "# Time the torch implementation\n",
        "def torch_time():\n",
        "    ans_torch(a_torch, w_torch)\n",
        "\n",
        "# Warm-up\n",
        "for _ in range(10):\n",
        "    torch_time()\n",
        "time_torch = timeit.timeit(torch_time, number=n_repeat)\n",
        "\n",
        "avg_time = time_torch / n_repeat\n",
        "\n",
        "print(f\"PyTorch conv2d (GPU) - Avg time over {n_repeat} runs: {avg_time * 1000:.4f} ms\")"
      ],
      "metadata": {
        "id": "tUPXWCdNi4oQ",
        "outputId": "4dc01941-d19c-446c-c667-d99c327e0497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch conv2d (GPU) - Avg time over 100 runs: 0.1733 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtM-xfNHVSoz",
        "outputId": "fdb00634-4490-4ffa-d00d-52d00b286ed8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer((3, 4, 16, 32), \"float32\"), W: T.Buffer((4, 1, 7, 7), \"float32\"), out: T.Buffer((3, 4, 16, 32), \"float32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
            "        inp_pad = T.allocate([10032], \"float32\", \"global\")\n",
            "        inp_pad_1 = T.Buffer((10032,), data=inp_pad)\n",
            "        for bb, cc in T.grid(3, 4):\n",
            "            blockIdx_y = T.launch_thread(\"blockIdx.y\", 6)\n",
            "            threadIdx_y = T.launch_thread(\"threadIdx.y\", 2)\n",
            "            for ww in range(38):\n",
            "                A_1 = T.Buffer((6144,), data=A.data)\n",
            "                inp_pad_1[bb * 3344 + cc * 836 + blockIdx_y * 152 + threadIdx_y * 38 + ww] = T.if_then_else(3 <= blockIdx_y * 4 + threadIdx_y and blockIdx_y * 4 + threadIdx_y < 19 and 3 <= ww and ww < 35, A_1[bb * 2048 + cc * 512 + blockIdx_y * 128 + threadIdx_y * 32 + ww - 99], T.float32(0))\n",
            "                if T.likely(blockIdx_y < 5):\n",
            "                    inp_pad_1[bb * 3344 + cc * 836 + blockIdx_y * 152 + threadIdx_y * 38 + ww + 76] = T.if_then_else(1 <= blockIdx_y * 4 + threadIdx_y and blockIdx_y * 4 + threadIdx_y < 17 and 3 <= ww and ww < 35, A_1[bb * 2048 + cc * 512 + blockIdx_y * 128 + threadIdx_y * 32 + ww - 35], T.float32(0))\n",
            "        for bb, cc in T.grid(3, 4):\n",
            "            blockIdx_x = T.launch_thread(\"blockIdx.x\", 32)\n",
            "            threadIdx_x = T.launch_thread(\"threadIdx.x\", 16)\n",
            "            out_1 = T.Buffer((6144,), data=out.data)\n",
            "            out_1[bb * 2048 + cc * 512 + blockIdx_x // 8 * 128 + threadIdx_x % 4 * 32 + blockIdx_x % 8 * 4 + threadIdx_x // 4] = T.float32(0)\n",
            "            for rkh, rkw in T.grid(7, 7):\n",
            "                W_1 = T.Buffer((196,), data=W.data)\n",
            "                out_1[bb * 2048 + cc * 512 + blockIdx_x // 8 * 128 + threadIdx_x % 4 * 32 + blockIdx_x % 8 * 4 + threadIdx_x // 4] = out_1[bb * 2048 + cc * 512 + blockIdx_x // 8 * 128 + threadIdx_x % 4 * 32 + blockIdx_x % 8 * 4 + threadIdx_x // 4] + inp_pad_1[bb * 3344 + cc * 836 + blockIdx_x // 8 * 152 + rkh * 38 + threadIdx_x % 4 * 38 + blockIdx_x % 8 * 4 + threadIdx_x // 4 + rkw] * W_1[cc * 49 + rkh * 7 + rkw]\n"
          ]
        }
      ],
      "source": [
        "print(tvm.lower(s, [inp, ker, out], simple_mode=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE2DD12GVSoz",
        "scrolled": false,
        "outputId": "af4526b9-30d4-4728-bbea-9c70edd3eb32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a3-BradyHuai\n",
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content/gdrive/MyDrive/ece5545/a3-BradyHuai\n",
            "plugins: anyio-4.9.0, typeguard-4.4.2, langsmith-0.3.23\n",
            "collected 192 items                                                                                \u001b[0m\n",
            "\n",
            "tests/test_dwsp_2dconv_gpu.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 32%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 80%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================= \u001b[32m\u001b[1m192 passed\u001b[0m\u001b[32m in 116.94s (0:01:56)\u001b[0m\u001b[32m ==================================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd {PROJECT_ROOT}\n",
        "!python -m pytest tests/test_dwsp_2dconv_gpu.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zqbo2VEVSo0"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"gh448@cornell.edu\"  # update with your email\n",
        "!git config --global user.name \"Guo Qing Huai\"   # update with your name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/ece5545/a3-BradyHuai/src\n",
        "!git commit -am \"benchmark runtime\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "cNHQmTsTkYHm",
        "outputId": "73ff7e8e-267d-4b17-f8cd-4546ed029da7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a3-BradyHuai/src\n",
            "[main c10bfb7] benchmark runtime\n",
            " 2 files changed, 5 insertions(+), 13 deletions(-)\n",
            "Enumerating objects: 11, done.\n",
            "Counting objects: 100% (11/11), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (6/6), done.\n",
            "Writing objects: 100% (6/6), 584 bytes | 64.00 KiB/s, done.\n",
            "Total 6 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/ML-HW-SYS/a3-BradyHuai.git\n",
            "   280691e..c10bfb7  main -> main\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5-conv2d_dw_gpu.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}